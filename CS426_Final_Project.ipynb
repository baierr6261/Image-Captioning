{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDcpcjokaias",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256814c5-34bf-4f76-d46b-17ced5b196a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "from keras.utils import to_categorical\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import PIL\n",
        "import os\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the images\n",
        "imagePath = \"/content/drive/My Drive/archive/Images/\"\n",
        "captionPath = \"/content/drive/My Drive/archive/captions.txt\"\n",
        "embeddingPath = \"/content/drive/My Drive/archive/glove.6B.200d.txt\"\n",
        "images = glob(imagePath + \"*.jpg\")"
      ],
      "metadata": {
        "id": "KwwmNKl2MMCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#quick visualization of a few images\n",
        "for i in range(5):\n",
        "  plt.figure()\n",
        "  image = cv2.imread(images[i])\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  plt.imshow(image)"
      ],
      "metadata": {
        "id": "sp-GDdswO36D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to load captions\n",
        "def load(fileName):\n",
        "  file = open(fileName, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "#this file is contains only the captions, the original file has two uneeded lines that cause issues\n",
        "file = \"/content/drive/My Drive/archive/captions1.txt\"\n",
        "info = load(file)\n",
        "print(info[:1])"
      ],
      "metadata": {
        "id": "yLZ15StMgOA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba8b4fa-32b0-45b3-9dd3-e546cca62c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a dictionary for the images and their respective captions\n",
        "def load_captions(info):\n",
        "  dict0 = dict()\n",
        "  for line in info.split('\\n'):\n",
        "    #print(line) #line seems to be correct\n",
        "    splitter = line.split('.jpg,')\n",
        "    #print(splitter) #splitter is fine\n",
        "\n",
        "    #image code and image captions are the list of the images and their respective captions\n",
        "    imageCode, imageCaption = splitter[0], splitter[1]\n",
        "\n",
        "    #creates the dictionary\n",
        "    if imageCode not in dict0:\n",
        "      dict0[imageCode] = list()\n",
        "\n",
        "    dict0[imageCode].append(imageCaption)\n",
        "\n",
        "  return dict0\n",
        "\n",
        "data = load_captions(info)"
      ],
      "metadata": {
        "id": "XtumWaWrPzcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing the data\n",
        "def cleanse_data(data):\n",
        "  dict0 = dict()\n",
        "  for key, value in data.items():\n",
        "    for i in range(len(value)):\n",
        "      lines = \"\"\n",
        "      line1 = value[i]\n",
        "      for j in line1.split():\n",
        "        if len(j) < 2:\n",
        "          continue\n",
        "        j = j.lower()\n",
        "        lines += j + \" \"\n",
        "      if key not in dict0:\n",
        "        dict0[key] = list()\n",
        "\n",
        "      dict0[key].append(lines)\n",
        "\n",
        "  return dict0\n",
        "\n",
        "data1 = cleanse_data(data)"
      ],
      "metadata": {
        "id": "pUycmOPPXnyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converts text into a vocabulary of words and calculates the words\n",
        "def vocab(data):\n",
        "  all_desc = set()\n",
        "  for key in data.keys():\n",
        "    [all_desc.update(d.split()) for d in data[key]]\n",
        "  return all_desc\n",
        "\n",
        "#summarizes the vocabulary\n",
        "vocabData = vocab(data1)"
      ],
      "metadata": {
        "id": "gsybaXI3Ynq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saves the descriptions to a different captions.txt file\n",
        "def save_dict(data, fileName):\n",
        "  lines = list()\n",
        "  for key, value in data.items():\n",
        "    for desc in value:\n",
        "      lines.append(key + ' ' + desc)\n",
        "  file = open(fileName, 'w')\n",
        "  file.write(str(data))\n",
        "  file.close()\n",
        "\n",
        "save_dict(data1, \"captions1.txt\")"
      ],
      "metadata": {
        "id": "Rp8wLfl7ZTGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocesses the images to for the inceptionv3 model\n",
        "def preprocessInception(imagePath):\n",
        "  #converts images to size 299x299\n",
        "  img = tf.keras.preprocessing.image.load_img(imagePath, target_size = (299, 299))\n",
        "\n",
        "  #converts image to 3D np array\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  #tutorial/code I'm using uses the inceptionV3 model to preprocess the input\n",
        "  x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "O1mrCgCbagAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocesses images for EfficientNetB0\n",
        "def preprocessEfficient(imagePath):\n",
        "  #converts images to size 224x224\n",
        "  img = tf.keras.preprocessing.image.load_img(imagePath, target_size = (224, 224))\n",
        "\n",
        "  #converts image to 3D np array\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "OGOr4DYYAKjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess images for ResNet50\n",
        "def preprocessResNet(imagePath):\n",
        "  #converts images to size 244x244\n",
        "  img = tf.keras.preprocessing.image.load_img(imagePath, target_size = (244, 244))\n",
        "\n",
        "  #converts image to 3D np array\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "#preprocess images for VGG19\n",
        "def preprocessVGG19(imagePath):\n",
        "  #converts images to size 244x244\n",
        "  img = tf.keras.preprocessing.image.load_img(imagePath, target_size = (244, 244))\n",
        "\n",
        "  #converts image to 3D np array\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x = tf.keras.applications.vgg19.preprocess_input(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "8yjNmKJdlwXp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new model based on inception v3 and using the imagenet weights\n",
        "input = tf.keras.applications.InceptionV3(weights = 'imagenet')\n",
        "#creates the new model and removes the output layer, in this case we're using inception v3\n",
        "model = tf.keras.models.Model(input.input, input.layers[-2].output)\n",
        "model.summary()\n",
        "\n",
        "#using EfficientNetB0\n",
        "effNetInput = tf.keras.applications.EfficientNetB0(weights = 'imagenet')\n",
        "model0 = tf.keras.models.Model(effNetInput.input, effNetInput.layers[-2].output)\n",
        "model0.summary()\n",
        "\n",
        "#using ResNet50\n",
        "resNetInput = tf.keras.applications.ResNet50V2(weights = 'imagenet')\n",
        "model50 = tf.keras.models.Model(resNetInput.input, resNetInput.layers[-2].output)\n",
        "model50.summary()\n",
        "\n",
        "#using VGG19\n",
        "vggInput = tf.keras.applications.VGG19(weights = 'imagenet')\n",
        "model19 = tf.keras.models.Model(vggInput.input, vggInput.layers[-2].output)\n",
        "model19.summary()\n"
      ],
      "metadata": {
        "id": "oeeslaCFfCLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encodes an image into a vector of size (2048, ) using inceptionv3\n",
        "def encodeInception(image):\n",
        "  image = preprocessInception(image)\n",
        "  fVec = model.predict(image)\n",
        "  fVec = np.reshape(fVec, fVec.shape[1])\n",
        "  return fVec\n",
        "\n",
        "encoding = {}\n",
        "\n",
        "for i in tqdm(images):\n",
        "  encoding[i[len(imagePath):]] = encodeInception(i)\n",
        "\n",
        "#puts the images into a pickle file\n",
        "with open(\"images1.pkl\", \"wb\") as encodedPickle:\n",
        "  pickle.dump(encoding, encodedPickle)"
      ],
      "metadata": {
        "id": "RoUW5KS5cvTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#same as above but using efficientnetB0\n",
        "def encodeEfficient(image):\n",
        "  image = preprocessEfficient(image)\n",
        "  fVec = model0.predict(image)\n",
        "  fVec = np.reshape(fVec, fVec.shape[1])\n",
        "  return fVec\n",
        "\n",
        "encoding = {}\n",
        "\n",
        "for i in tqdm(images):\n",
        "  encoding[i[len(imagePath):]] = encodeEfficient(i)\n",
        "\n",
        "#making a 2nd pickle file\n",
        "with open(\"images2.pkl\", \"wb\") as encodedPickle:\n",
        "  pickle.dump(encoding, encodedPickle)"
      ],
      "metadata": {
        "id": "j4ah6WGbloHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding images w/ resnet50\n",
        "def encodeResNet(image):\n",
        "  image = preprocessResNet(image)\n",
        "  fVec = model50.predict(image)\n",
        "  fVec = np.reshape(fVec, fVec.shape[1])\n",
        "  return fVec\n",
        "\n",
        "encoding = {}\n",
        "\n",
        "for i in tqdm(images):\n",
        "  encoding[i[len(imagePath):]] = encodeResNet(i)\n",
        "\n",
        "with open(\"images3.pkl\", \"wb\") as encodedPickle:\n",
        "  pickle.dump(encoding, encodedPickle)"
      ],
      "metadata": {
        "id": "drYJNQUgOsrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encodes images using vgg19\n",
        "def encodeVGG(image):\n",
        "  image = preprocessVGG19(image)\n",
        "  fVec = model19.predict(image)\n",
        "  fVec = np.reshape(fVec, fVec.shape[1])\n",
        "  return fVec\n",
        "\n",
        "encoding = {}\n",
        "\n",
        "for i in tqdm(images):\n",
        "  encoding[i[len(imagePath):]] = encodeVGG(i)\n",
        "\n",
        "with open(\"images4.pkl\", \"wb\") as encodedPickle:\n",
        "  pickle.dump(encoding, encodedPickle)"
      ],
      "metadata": {
        "id": "KUT_WVD-P1uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a list of all training captions\n",
        "trainingCaptions = []\n",
        "for key, val in data1.items():\n",
        "  for caption in val:\n",
        "    trainingCaptions.append(caption)\n",
        "\n",
        "len(trainingCaptions)\n",
        "\n",
        "#considers only words which occurs at least 10 times\n",
        "threshold = 10\n",
        "wordCounts = {}\n",
        "nsents = 0\n",
        "\n",
        "for sent in trainingCaptions:\n",
        "  nsents += 1\n",
        "  for w in sent.split(' '):\n",
        "    wordCounts[w] = wordCounts.get(w, 0) + 1\n",
        "\n",
        "vocabulary = [w for w in wordCounts if wordCounts[w] >= threshold]\n",
        "print('preprocessed words %d -> %d' % (len(wordCounts), len(vocabulary)))\n",
        "\n",
        "#converts the words to indices and vice versa\n",
        "indexWord = {}\n",
        "wordIndex = {}\n",
        "\n",
        "index = 1\n",
        "for w in vocabulary:\n",
        "  wordIndex[w] = index\n",
        "  indexWord[index] = w\n",
        "  index += 1\n",
        "\n",
        "vocabSize = len(indexWord) + 1\n",
        "\n",
        "#tutoral I'm using says to convert the 2 above dictionaries(?) into pickle files, again I'll ignore that\n",
        "\n",
        "#converts a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(desc):\n",
        "  allDesc = list()\n",
        "  for key in desc.keys():\n",
        "    [allDesc.append(d) for d in desc[key]]\n",
        "  return allDesc\n",
        "\n",
        "#calculates the length of the description w/ the most words, not sure if needed\n",
        "def max_length(desc):\n",
        "  lines = to_lines(desc)\n",
        "  return max(len(d.split()) for d in lines)\n",
        "\n",
        "#determines the max. sequence length, used as a parameter for data_generator(...)\n",
        "maxLength = max_length(data1)\n",
        "print('Description Length: %d' % maxLength)"
      ],
      "metadata": {
        "id": "Xi9KWcHWglpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data generator function to be used as a parameter for model.fit_generator(), or model.fit()\n",
        "def data_generator(descriptions, photos, wordIndex, maxLength, photoNumPerBatch):\n",
        "  x1, x2, y = list(), list(), list()\n",
        "  n = 0\n",
        "  #loops over each image\n",
        "  while 1:\n",
        "    for key, descList in descriptions.items():\n",
        "      n += 1\n",
        "      #retrives the photo feature\n",
        "      photo = photos[key + '.jpg']\n",
        "      for desc in descList:\n",
        "        #encodes the sequence\n",
        "        seq = [wordIndex[word] for word in desc.split(' ') if word in wordIndex]\n",
        "        #splits a sequence into multiple x, y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "          #splits into input and output pairs\n",
        "          inSeq, outSeq = seq[:i], seq[i]\n",
        "          #pads the input sequence\n",
        "          inSeq = pad_sequences([inSeq], maxlen = maxLength)[0]\n",
        "          #encodes the output sequence\n",
        "          outSeq = to_categorical([outSeq], num_classes = vocabSize)[0]\n",
        "          #stores the values into their respective lists\n",
        "          x1.append(photo)\n",
        "          x2.append(inSeq)\n",
        "          y.append(outSeq)\n",
        "\n",
        "      #yield batch data\n",
        "      if n == photoNumPerBatch:\n",
        "        yield([array(x1), array(x2)], array(y))\n",
        "        x1, x2, y = list(), list(), list()\n",
        "        n = 0"
      ],
      "metadata": {
        "id": "JgdYud6mjtGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a word embedding vector for each unique word for a fixed length\n",
        "#using glove.6B.200d.txt from https://github.com/stanfordnlp/GloVe\n",
        "#otherwise, I think we would need to create a custom embedding vector\n",
        "embeddingIndex = {}\n",
        "f = open(embeddingPath, encoding = \"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype = 'float32')\n",
        "  embeddingIndex[word] = coefs\n",
        "f.close()\n",
        "\n",
        "#print('Found %s word vectors.' % len(embeddingIndex))\n",
        "\n",
        "embeddingDim = 200\n",
        "embeddingMatrix = np.zeros((vocabSize, embeddingDim))\n",
        "\n",
        "for word, i in wordIndex.items():\n",
        "  embeddingVector = embeddingIndex.get(word)\n",
        "  if embeddingVector is not None:\n",
        "    embeddingMatrix[i] = embeddingVector\n",
        "\n",
        "#print(embeddingMatrix.shape)"
      ],
      "metadata": {
        "id": "HQRY_G3_gW8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#additonal encoding/decoding layers initially for inceptionv3, need to change the model variable if using a different encoding model\n",
        "input1 = tf.keras.layers.Input(shape = (2048, ))\n",
        "fe1 = tf.keras.layers.Dropout(0.5)(input1)\n",
        "fe2 = tf.keras.layers.Dense(256, activation = 'relu')(fe1)\n",
        "\n",
        "input2 = tf.keras.layers.Input(shape = (maxLength,))\n",
        "se1 = tf.keras.layers.Embedding(vocabSize, embeddingDim, mask_zero = True)(input2)\n",
        "se2 = tf.keras.layers.Dropout(0.5)(se1)\n",
        "se3 = tf.keras.layers.GRU(256)(se2)#se3 = tf.keras.layers.LSTM(256)(se2)\n",
        "\n",
        "decoder1 = tf.keras.layers.add([fe2, se3])\n",
        "decoder2 = tf.keras.layers.Dense(256, activation = 'relu')(decoder1)\n",
        "output = tf.keras.layers.Dense(vocabSize, activation = 'softmax')(decoder2)\n",
        "\n",
        "model3 = tf.keras.models.Model(inputs = [input1, input2], outputs = output)\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "PGPslv94gYNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the inceptionv3 model\n",
        "model3.layers[2].set_weights([embeddingMatrix])\n",
        "model3.layers[2].trainable = False\n",
        "\n",
        "model3.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "epochs = 10\n",
        "batchNum = 3\n",
        "steps = len(data1) // batchNum // 100\n",
        "\n",
        "#tutorial uses the pickle file here as the features\n",
        "features = pickle.load(open(\"/content/images1.pkl\", \"rb\"))\n",
        "\n",
        "#stops an error where function tries to create variables on non-first call\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "generator = data_generator(data1, features, wordIndex, maxLength, batchNum)\n",
        "model3.fit(generator, steps_per_epoch = steps, epochs = epochs, verbose = 1)\n",
        "#model3.save('model3.h5')"
      ],
      "metadata": {
        "id": "PcmnsI7mgZrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trains efficientnetB0 model\n",
        "steps = len(data1) // 300\n",
        "model0.layers[2].set_weights([embeddingMatrix])\n",
        "model0.layers[2].trainable = False\n",
        "\n",
        "model0.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "features = pickle.load(open(\"/content/images2.pkl\", \"rb\"))\n",
        "\n",
        "#stops an error where function tries to create variables on non-first call\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "generator = data_generator(data1, features, wordIndex, maxLength, 3)\n",
        "model0.fit(generator, steps_per_epoch = steps, epochs = 10, verbose = 1)\n",
        "model0.save('model0.h5')"
      ],
      "metadata": {
        "id": "jbcg2T0NB0GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trains resnet50 model\n",
        "steps = len(data1) // 300\n",
        "model50.layers[2].set_weights([embeddingMatrix])\n",
        "model50.layers[2].trainable = False\n",
        "\n",
        "model50.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "features = pickle.load(open(\"/content/images3.pkl\", \"rb\"))\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "generator = data_generator(data1, features, wordIndex, maxLength, 3)\n",
        "model50.fit(generator, steps_per_epoch = steps, epochs = 10, verbose = 1)\n",
        "model50.save('model50.h5')"
      ],
      "metadata": {
        "id": "MjrDu9DHRFdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trains vgg19 model\n",
        "steps = len(data1) // 300\n",
        "model19.layers[2].set_weights([embeddingMatrix])\n",
        "model19.layers[2].trainable = False\n",
        "\n",
        "model19.compile(loss = 'categorical_crossentropy', optimizer = 'adam)\n",
        "\n",
        "features = pickle.load(open(\"/content/images4.pkl\", \"rb\"))\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "generator = data_generator(data1, features, wordIndex, maxLength, 3)\n",
        "model19.fit(generator, steps_per_epoch = steps, epochs = 10, verbose = 1)\n",
        "model19.save('model19.h5')"
      ],
      "metadata": {
        "id": "Ea3CgiU-SHq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making predictions w/ the model\n",
        "def image_caption(picture):\n",
        "  in_text = 'startseq'\n",
        "  for i in range(maxLength):\n",
        "    seq = [wordIndex[w] for w in in_text.split() if w in wordIndex]\n",
        "    seq = pad_sequences([seq], maxlen = maxLength)\n",
        "    yhat = model0.predict([picture, seq], verbose = 0)\n",
        "    yhat = np.argmax(yhat)\n",
        "    word = indexWord[yhat]\n",
        "    in_text += ' ' + word\n",
        "    if word == 'endseq':\n",
        "      break\n",
        "  final = in_text.split()\n",
        "  final = final[1:-1]\n",
        "  final = ' '.join(final)\n",
        "  return final"
      ],
      "metadata": {
        "id": "2ZkvXJ_2q1QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#randomly selects an image then attemps to visualize the output using the model\n",
        "z = 78 #20\n",
        "pic = list(features.keys())[z]\n",
        "image = features[pic].reshape((1, 4096))\n",
        "#image = features[pic].reshape((1, 2048))\n",
        "#image = features[pic].reshape((1, 1280))\n",
        "x = plt.imread(imagePath + pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "print(\"Caption: \", image_caption(image))"
      ],
      "metadata": {
        "id": "FA3Y5PZtQBrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 587 #200\n",
        "pic = list(features.keys())[z]\n",
        "image = features[pic].reshape((1, 4096))\n",
        "#image = features[pic].reshape((1, 2048))\n",
        "#image = features[pic].reshape((1, 1280))\n",
        "x = plt.imread(imagePath + pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "print(\"Caption: \", image_caption(image))"
      ],
      "metadata": {
        "id": "Flz3CkCYQkKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[:-100] because early stopping restored best values (100 before it stopped)\n",
        "\n",
        "plt.plot(history['loss'][:-100])\n",
        "plt.plot(history['val_loss'][:-100])\n",
        "plt.title('Loss Graph for InceptionV3')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'validation'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VH7eLNYL_EYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['categorical_accuracy'][:-100])\n",
        "plt.plot(history['val_categorical_accuracy'][:-100])\n",
        "plt.title('Accuracy Graph for InceptionV3')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'validation'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XcR4jWxZ_FKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}